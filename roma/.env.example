# ROMA Environment Configuration
# Copy this file to .env and add API keys for the providers you plan to use
#
# IMPORTANT: You only need to set API keys for the LLM providers you actually use in your configuration.
# ROMA will only require API keys when creating models that need them.

# =============================================================================
# LLM Provider API Keys (Set only what you need)
# =============================================================================

# OpenAI - Required ONLY if using GPT models (gpt-4, gpt-3.5-turbo, etc.)
# OPENAI_API_KEY=sk-your-openai-api-key-here

# Anthropic - Required ONLY if using Claude models (claude-3-5-sonnet, claude-3-haiku, etc.)
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# =============================================================================
# Optional LLM Providers (Uncomment and set only if using these models)
# =============================================================================

# Google AI - Required ONLY if using Gemini models (gemini-pro, gemini-1.5-pro, etc.)
# GOOGLE_API_KEY=your-google-api-key-here

# OpenRouter - Required ONLY if using OpenRouter-hosted models
# OPENROUTER_API_KEY=your-openrouter-api-key-here

# Together AI - Required ONLY if using Together AI models
# TOGETHER_API_KEY=your-together-api-key-here

# Groq - Required ONLY if using Groq models (llama3-70b-8192, etc.)
# GROQ_API_KEY=your-groq-api-key-here

# Cohere - Required ONLY if using Cohere models (command, command-r, etc.)
# COHERE_API_KEY=your-cohere-api-key-here

# Replicate - Required ONLY if using Replicate models
# REPLICATE_API_TOKEN=your-replicate-token-here

# Hugging Face - Required ONLY if using HF-hosted models
# HUGGINGFACE_API_TOKEN=your-huggingface-token-here

# =============================================================================
# AWS Bedrock (Optional - Uncomment only if using AWS Bedrock models)
# =============================================================================

# AWS credentials - Required ONLY if using Bedrock models (claude-v2, llama2-13b-chat-v1, etc.)
# AWS_ACCESS_KEY_ID=your-aws-access-key-id
# AWS_SECRET_ACCESS_KEY=your-aws-secret-access-key
# AWS_REGION=us-east-1

# =============================================================================
# ROMA Configuration
# =============================================================================

# Logging level (DEBUG, INFO, WARNING, ERROR)
ROMA_LOG_LEVEL=INFO

# Maximum concurrent tasks
ROMA_MAX_CONCURRENT_TASKS=10

# Task timeout in seconds
ROMA_TASK_TIMEOUT=120

# Total execution timeout in seconds
ROMA_TOTAL_TIMEOUT=600

# Enable caching
ROMA_ENABLE_CACHING=true

# Cache TTL in seconds
ROMA_CACHE_TTL=1800

# =============================================================================
# Development/Testing
# =============================================================================

# Set to "true" to enable test mode (uses mock responses)
ROMA_TEST_MODE=false

# Prompt template directory override
# ROMA_PROMPTS_DIR=/custom/path/to/prompts